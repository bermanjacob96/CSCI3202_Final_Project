{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Jacob Berman\n",
    "CSCI 3202\n",
    "Cart-Pole Q-Learning\n",
    "'''\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRewards = []\n",
    "timeReq  = []\n",
    "meanScores = []\n",
    "meanScores200 = []\n",
    "\n",
    "class Qlearn:\n",
    "    \n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), nEpisodes=1000, nWins=195, minAlpha=0.1, minEpsilon=0.1,\n",
    "                 gamma=1.0, adaDivisor=25, maxEnvSteps=None, quiet=False, monitor=False):\n",
    "        \n",
    "        self.buckets = buckets\n",
    "        self.nEpisodes = nEpisodes\n",
    "        self.nWins = nWins\n",
    "        self.minAlpha = minAlpha\n",
    "        self.minEpsilon = minEpsilon\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.adaDivisor = adaDivisor\n",
    "        self.quiet = quiet\n",
    "        \n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        if maxEnvSteps is not None:\n",
    "            self.env.maxEpisodeSteps = maxEnvSteps\n",
    "            \n",
    "        if monitor:\n",
    "            self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True)\n",
    "                \n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,)) #Initialize Q-table\n",
    "        \n",
    "    \n",
    "    def discretize(self, obs):\n",
    "        \n",
    "        upperBounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lowerBounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        \n",
    "        ratios = [(obs[i] + abs(lowerBounds[i])) / (upperBounds[i] - lowerBounds[i]) for i in range(len(obs))]\n",
    "        \n",
    "        newObsv = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        newObsv = [min(self.buckets[i] - 1, max(0, newObsv[i])) for i in range(len(obs))]\n",
    "        \n",
    "        return tuple(newObsv)\n",
    "            \n",
    "    def chooseAction(self, state, epsilon):\n",
    "        \n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "    \n",
    "    def updateQ(self, stateOld, action, reward, stateNew, alpha):\n",
    "        \n",
    "        self.Q[stateOld][action] += alpha * (reward + self.gamma * np.max(self.Q[stateNew]) - self.Q[stateOld][action])\n",
    "        \n",
    "    def getEpsilon(self, t):\n",
    "        \n",
    "        return max(self.minEpsilon, min(1, 1.0 - math.log10((t + 1) / self.adaDivisor)))\n",
    "    \n",
    "    def getAlpha(self, t):\n",
    "        \n",
    "        return max(self.minAlpha, min(1.0, 1.0 - math.log10((t + 1) / self.adaDivisor)))\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        scores = deque(maxlen=100)\n",
    "        \n",
    "        for e in range(self.nEpisodes):\n",
    "            \n",
    "            currentState = self.discretize(self.env.reset())\n",
    "            \n",
    "            alpha = self.getAlpha(e)\n",
    "            epsilon = self.getEpsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "            \n",
    "            for time in range(500):\n",
    "                \n",
    "                self.env.render()\n",
    "                action = self.chooseAction(currentState, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                newState = self.discretize(obs)\n",
    "                self.updateQ(currentState, action, reward, newState, alpha)\n",
    "                currentState = newState\n",
    "                i += reward\n",
    "                \n",
    "                if done:\n",
    "                    print('Episode:{}/{} finished with reward:{}'.format(e, self.nEpisodes, time))\n",
    "                    break\n",
    "                \n",
    "            scores.append(i)\n",
    "            finalRewards.append(i)\n",
    "            \n",
    "            meanScore = np.mean(scores)\n",
    "            meanScore200 = np.mean(scores)\n",
    "\n",
    "            \n",
    "            if meanScore >= self.nWins and e >= 100:\n",
    "                if not self.quiet: print('\\nRan {} episodes. Solved after {} episodes'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            \n",
    "            if e == 100:\n",
    "                print('\\n[Episode {}] - Mean time over last 100 episodes was {} ticks.'.format(e, meanScore))\n",
    "                meanScores.append(meanScore)\n",
    "                #print(meanScores)\n",
    "            if e == 200:\n",
    "                print('\\n[Episode {}] - Mean time over last 100 episodes was {} ticks.'.format(e, meanScore200))\n",
    "                meanScores200.append(meanScore200)\n",
    "                #print(meanScores200)\n",
    "\n",
    "\n",
    "        if not self.quiet: print('\\nDid not solve after {} episodes'.format(e))\n",
    "        return e\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    qlearn = Qlearn()\n",
    "    qlearn.run()\n",
    "    #for (x) in range(10): //Uncomment to run 10 tests. Commented out becuase it messes with other graphs.\n",
    "    #    qlearn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(finalRewards)\n",
    "plt.title('Q-Learning Performance')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(meanScores)\n",
    "plt.title('Mean scores for 0-100')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(meanScores200)\n",
    "plt.title('Mean scores for 100-200')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
